{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "\n",
    "All preprocessing that does not implement any fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from capstone_project import preprocessor as pre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tokenized datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current working directory for python is the capstone_project/notebook folder\n",
    "file_directory = \"../output/data/\"\n",
    "prefix = \"\"\n",
    "\n",
    "train_data = pre.load_pickle(file_directory, prefix+\"train_data.pkl\")\n",
    "y = train_data[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if correct dataframe has been loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_tokens</th>\n",
       "      <th>q2_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355802</th>\n",
       "      <td>355802</td>\n",
       "      <td>696825</td>\n",
       "      <td>696826</td>\n",
       "      <td>Which are the best songs of Enrique Iglesias?</td>\n",
       "      <td>Which is the best song of Enrique iglesias?</td>\n",
       "      <td>1</td>\n",
       "      <td>[best, song, enrique, iglesias]</td>\n",
       "      <td>[best, song, enrique, iglesias]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2                                      question1  \\\n",
       "355802  355802  696825  696826  Which are the best songs of Enrique Iglesias?   \n",
       "\n",
       "                                          question2  is_duplicate  \\\n",
       "355802  Which is the best song of Enrique iglesias?             1   \n",
       "\n",
       "                              q1_tokens                        q2_tokens  \n",
       "355802  [best, song, enrique, iglesias]  [best, song, enrique, iglesias]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_data.head(1))\n",
    "train_data = train_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained word2vec model. The model can be downloaded at: #TODO find url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will create the transformers and add the to a pipeline that we will use to create the new features. This is more work upfront and might seem like overkill since we could just apply the transformations on the dataframes directly, but I tend to play around with the feature engineering and I find that this way my features are better organized, which saves time in the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = pre.FeatureTransformer()\n",
    "word2vec_transform = pre.Word2vecTransformer(word2vec_model, sum_up=True)\n",
    "word2vec_transform = pre.Word2vecTransformer()\n",
    "word2vec_features = pre.VectorFeatureTransformer()\n",
    "mms = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_jobs', 'word2vec_pipeline', 'word2vec_pipeline__vec_transformer__model', 'transformer_weights', 'word2vec_pipeline__steps', 'word2vec_pipeline__vec_transformer', 'word2vec_pipeline__vec_features', 'transformer_list', 'word_features', 'word2vec_pipeline__vec_transformer__sum_up']\n"
     ]
    }
   ],
   "source": [
    "word2vec_pipe = Pipeline([(\"vec_transformer\", word2vec_transform), (\"vec_features\", word2vec_features)])\n",
    "feature_creator = FeatureUnion([('word_features', word_features), ('word2vec_pipeline', word2vec_pipe)])\n",
    "\n",
    "print feature_creator.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('word_features', FeatureTransformer()), ('word2vec_pipeline', Pipeline(steps=[('vec_transformer', Word2vecTransformer(model=<gensim.models.keyedvectors.KeyedVectors object at 0x7f8038a2c150>,\n",
       "          sum_up=True)), ('vec_features', VectorFeatureTransformer())]))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params = {\"word2vec_pipeline__vec_transformer__sum_up\": True, \n",
    "                  \"word2vec_pipeline__vec_transformer__model\": word2vec_model}\n",
    "feature_creator.set_params(**initial_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_creator.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 45.          43.           2.           4.           4.          11.25\n",
      "  10.75         0.75         0.           0.           0.           0.           0.\n",
      "   0.           0.          -0.11522716  -0.11522716  -0.24668134\n",
      "  -0.24668134] <type 'numpy.ndarray'>\n",
      "['q1_length' 'q2_length' 'diff_length' 'q1_n_words' 'q2_n_words'\n",
      " 'q1_len_word_ratio' 'q2_len_word_ratio' 'word_share'] ['word2vec_cosine_distance' 'word2vec_cityblock_distance'\n",
      " 'word2vec_jaccard_distance' 'word2vec_canberra_distance'\n",
      " 'word2vec_minkowski_distance' 'word2vec_euclidean_distance'\n",
      " 'word2vec_braycurtis_distance' 'word2vec_skew_q1' 'word2vec_skew_q2'\n",
      " 'word2vec_kurtosis_q1' 'word2vec_kurtosis_q2']\n"
     ]
    }
   ],
   "source": [
    "print features[0],type(features)\n",
    "print word_features.get_feature_names(), word2vec_features.get_feature_names()\n",
    "#print train_data.columns.values, type( train_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
