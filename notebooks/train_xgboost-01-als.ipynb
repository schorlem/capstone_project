{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import xgboost as xgb\n",
    "\n",
    "from capstone_project import preprocessor as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pre.load_data()\n",
    "y = train_data[\"is_duplicate\"].values\n",
    "model_directory = \"../output/models/\"\n",
    "skf = pre.load_pickle(model_directory, \"kfolds.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, x_train, x_test, y_train, y_test, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \"\"\"https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\"\"\"\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        #xgtest = xgb.DMatrix(x_test)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                          metrics='logloss', early_stopping_rounds=early_stopping_rounds, stratified=True)\n",
    "        print(\"\\nBest number of iterations: {}\\n\".format(cvresult.shape[0]))\n",
    "        print(cvresult[-10:])\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "\n",
    "    # Fit the algorithm on the data\n",
    "    alg.fit(x_train, y_train, eval_metric='logloss')\n",
    "\n",
    "    # Predict training set:\n",
    "    train_predictions = alg.predict(x_train)\n",
    "    train_predprob = alg.predict_proba(x_train)[:, 1]\n",
    "\n",
    "    # Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_train, train_predictions))\n",
    "    print(\"Logloss (Train): %f\" % metrics.log_loss(y_train, train_predprob))\n",
    "\n",
    "    #     Predict on testing data:\n",
    "    test_pred = alg.predict_proba(x_test)[:, 1]\n",
    "    print('Logloss (Test): %f' % metrics.log_loss(y_test, test_pred))\n",
    "\n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_features = pre.FeatureTransformer()\n",
    "word2vec_transform = pre.Word2vecTransformer()\n",
    "word2vec_features = pre.VectorFeatureTransformer()\n",
    "mms = MinMaxScaler()\n",
    "clf = xgboost()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
