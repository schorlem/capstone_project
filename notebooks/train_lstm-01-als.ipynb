{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Train LSTM with word2vec embeddings </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I would like to mention these two excellent posts: \n",
    "\n",
    "https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "My solution is in part based on these two guidelines.\n",
    "\n",
    "Okay now let's import all of the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from unidecode import unidecode\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from capstone_project import preprocessor as pre\n",
    "from capstone_project.models import neural_nets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set important constants and load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50 # Maximum length of input for lstm the maximum number of tokens is 103 \n",
    "EMBEDDING_DIM = 300  # Length of the used word2vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_directory = \"../output/data/\"\n",
    "prefix = \"tokenized_\"\n",
    "\n",
    "train_data = pre.load_pickle(file_directory, prefix+\"train_data.pkl\")\n",
    "val_data = pre.load_pickle(file_directory, prefix+\"val_data.pkl\")  # Validation data set used to compare different classification algorithms\n",
    "train_y = train_data[\"is_duplicate\"].values\n",
    "val_y = val_data[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_data = train_data[:100]\n",
    "#val_data = val_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the tokenized question as input for keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66580 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# Decode again and join strings because keras tokenizer crashes when using unicode while spacy uses it\n",
    "q1_tokens = train_data[\"q1_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "q2_tokens = train_data[\"q2_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "all_tokens = np.concatenate([q1_tokens, q2_tokens])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_tokens)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "number_words = len(word_index)+1  # Needed for embedding layer\n",
    "print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "\n",
    "q1_sequences = tokenizer.texts_to_sequences(q1_tokens)\n",
    "q2_sequences = tokenizer.texts_to_sequences(q2_tokens)\n",
    "\n",
    "q1_data = pad_sequences(q1_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(q2_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split second val and train set for validation at every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into another training set and a second validation set \n",
    "# See: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html and\n",
    "indices = np.arange(q1_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "q1_data = q1_data[indices]\n",
    "q2_data = q2_data[indices]\n",
    "labels = train_y[indices]\n",
    "\n",
    "nb_validation_samples = int(0.1 * q1_data.shape[0])\n",
    "\n",
    "# Create subset for training with early stopping\n",
    "q1_train = q1_data[:-nb_validation_samples]\n",
    "q2_train = q1_data[:-nb_validation_samples]\n",
    "train_labels = labels[:-nb_validation_samples]\n",
    "\n",
    "#create validation subset that is used to validate each epoch during training\n",
    "q1_val_epochs = q1_data[-nb_validation_samples:]\n",
    "q2_val_epochs = q2_data[-nb_validation_samples:]\n",
    "val_epochs_labels = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doble the dataset size by switching the order of the questions. This is done in order to avoid symmetry issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kaggle ...\n",
    "q12_data = np.concatenate((q1_data, q2_data), axis=0)\n",
    "q21_data = np.concatenate((q2_data, q1_data), axis=0)\n",
    "double_labels = np.concatenate((labels, labels), axis=0)\n",
    "\n",
    "q12_train = np.concatenate((q1_train, q2_train), axis=0)\n",
    "q21_train = np.concatenate((q2_train, q1_train), axis=0)\n",
    "double_train_labels = np.concatenate((train_labels, train_labels), axis=0)\n",
    "\n",
    "q12_val_epochs = np.concatenate((q1_val_epochs, q2_val_epochs), axis=0)\n",
    "q21_val_epochs = np.concatenate((q2_val_epochs, q1_val_epochs), axis=0)\n",
    "double_val_epochs_labels = np.concatenate((val_epochs_labels, val_epochs_labels), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create correct embeddings for validation data\n",
    "q1_val_tokens = val_data[\"q1_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "q2_val_tokens = val_data[\"q2_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "\n",
    "q1_val_sequences = tokenizer.texts_to_sequences(q1_val_tokens)\n",
    "q2_val_sequences = tokenizer.texts_to_sequences(q2_val_tokens)\n",
    "\n",
    "q1_val_data = pad_sequences(q1_val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_val_data = pad_sequences(q2_val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val_labels = val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained word2vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters of the lstm, create the emebedding matrix and create a keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 30612\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 50, 300)       19974300    input_5[0][0]                    \n",
      "                                                                   input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 256)           570368      embedding_3[0][0]                \n",
      "                                                                   embedding_3[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 512)           0           lstm_3[0][0]                     \n",
      "                                                                   lstm_3[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 512)           2048        concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 512)           0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 256)           131328      dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 256)           1024        dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 256)           0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 128)           32896       dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 128)           512         dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 128)           0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1)             129         dropout_9[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 20,712,605\n",
      "Trainable params: 736,513\n",
      "Non-trainable params: 19,976,092\n",
      "____________________________________________________________________________________________________\n",
      "256_0.50_[256, 128]_0.50\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "nn_parameters = {\"max_sequence_length\": MAX_SEQUENCE_LENGTH,\n",
    "                 \"num_lstm\": 256,\n",
    "                 \"dropout_lstm\": 0.5,\n",
    "                 \"num_dense\": [256, 128],\n",
    "                 \"dropout_dense\": 0.5}\n",
    "\n",
    "stamp = \"{}_{:.2f}_{}_{:.2f}\".format(nn_parameters[\"num_lstm\"], \n",
    "                                    nn_parameters[\"dropout_lstm\"],\n",
    "                                    nn_parameters[\"num_dense\"],\n",
    "                                    nn_parameters[\"dropout_dense\"])\n",
    "\n",
    "embedding_matrix = neural_nets.create_embedding_matrix(word2vec_model, EMBEDDING_DIM, word_index, number_words)\n",
    "\n",
    "model = neural_nets.create_lstm(embedding_matrix, EMBEDDING_DIM, number_words, **nn_parameters)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model.summary()\n",
    "print stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the data and check the performance on the second validation set every epoch. with early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 524038 samples, validate on 58226 samples\n",
      "Epoch 1/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.6467 - acc: 0.6473 - val_loss: 0.6126 - val_acc: 0.6389\n",
      "Epoch 2/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.5868 - acc: 0.6894 - val_loss: 0.5639 - val_acc: 0.7141\n",
      "Epoch 3/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.5651 - acc: 0.7081 - val_loss: 0.5388 - val_acc: 0.7270\n",
      "Epoch 4/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.5507 - acc: 0.7197 - val_loss: 0.5274 - val_acc: 0.7372\n",
      "Epoch 5/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.5403 - acc: 0.7273 - val_loss: 0.5186 - val_acc: 0.7438\n",
      "Epoch 6/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.5314 - acc: 0.7331 - val_loss: 0.5141 - val_acc: 0.7460\n",
      "Epoch 7/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.5248 - acc: 0.7380 - val_loss: 0.5090 - val_acc: 0.7489\n",
      "Epoch 8/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.5180 - acc: 0.7428 - val_loss: 0.5051 - val_acc: 0.7526\n",
      "Epoch 9/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.5114 - acc: 0.7475 - val_loss: 0.5016 - val_acc: 0.7544\n",
      "Epoch 10/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.5060 - acc: 0.7510 - val_loss: 0.4977 - val_acc: 0.7572\n",
      "Epoch 11/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.5010 - acc: 0.7542 - val_loss: 0.4964 - val_acc: 0.7579\n",
      "Epoch 12/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.4956 - acc: 0.7580 - val_loss: 0.4957 - val_acc: 0.7585\n",
      "Epoch 13/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.4898 - acc: 0.7614 - val_loss: 0.4908 - val_acc: 0.7625\n",
      "Epoch 14/300\n",
      "524038/524038 [==============================] - 322s - loss: 0.4866 - acc: 0.7644 - val_loss: 0.4918 - val_acc: 0.7613\n",
      "Epoch 15/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.4820 - acc: 0.7664 - val_loss: 0.4898 - val_acc: 0.7628\n",
      "Epoch 16/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.4784 - acc: 0.7693 - val_loss: 0.4882 - val_acc: 0.7644\n",
      "Epoch 17/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.4743 - acc: 0.7712 - val_loss: 0.4880 - val_acc: 0.7674\n",
      "Epoch 18/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.4706 - acc: 0.7738 - val_loss: 0.4879 - val_acc: 0.7648\n",
      "Epoch 19/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.4669 - acc: 0.7757 - val_loss: 0.4867 - val_acc: 0.7686\n",
      "Epoch 20/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.4645 - acc: 0.7770 - val_loss: 0.4860 - val_acc: 0.7667\n",
      "Epoch 21/300\n",
      "524038/524038 [==============================] - 322s - loss: 0.4615 - acc: 0.7788 - val_loss: 0.4884 - val_acc: 0.7673\n",
      "Epoch 22/300\n",
      "524038/524038 [==============================] - 322s - loss: 0.4585 - acc: 0.7811 - val_loss: 0.4878 - val_acc: 0.7671\n",
      "Epoch 23/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.4570 - acc: 0.7810 - val_loss: 0.4856 - val_acc: 0.7676\n",
      "Epoch 24/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.4538 - acc: 0.7836 - val_loss: 0.4844 - val_acc: 0.7688\n",
      "Epoch 25/300\n",
      "524038/524038 [==============================] - 323s - loss: 0.4518 - acc: 0.7848 - val_loss: 0.4830 - val_acc: 0.7706\n",
      "Epoch 26/300\n",
      "524038/524038 [==============================] - 322s - loss: 0.4492 - acc: 0.7862 - val_loss: 0.4863 - val_acc: 0.7696\n",
      "Epoch 27/300\n",
      "524038/524038 [==============================] - 322s - loss: 0.4477 - acc: 0.7875 - val_loss: 0.4841 - val_acc: 0.7701\n",
      "Epoch 28/300\n",
      "524038/524038 [==============================] - 322s - loss: 0.4454 - acc: 0.7885 - val_loss: 0.4856 - val_acc: 0.7708\n",
      "Epoch 29/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.6122 - acc: 0.6623 - val_loss: 0.6421 - val_acc: 0.6310\n",
      "Epoch 30/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.6431 - acc: 0.6367 - val_loss: 0.5977 - val_acc: 0.6762\n",
      "Epoch 31/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.6094 - acc: 0.6669 - val_loss: 0.5824 - val_acc: 0.6923\n",
      "Epoch 32/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5856 - acc: 0.6879 - val_loss: 0.5373 - val_acc: 0.7301\n",
      "Epoch 33/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5622 - acc: 0.7077 - val_loss: 0.5294 - val_acc: 0.7369\n",
      "Epoch 34/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5508 - acc: 0.7174 - val_loss: 0.5268 - val_acc: 0.7364\n",
      "Epoch 35/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5441 - acc: 0.7219 - val_loss: 0.5222 - val_acc: 0.7394\n",
      "Epoch 36/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5693 - acc: 0.7016 - val_loss: 0.5400 - val_acc: 0.7252\n",
      "Epoch 37/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5734 - acc: 0.6984 - val_loss: 0.5346 - val_acc: 0.7332\n",
      "Epoch 38/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5756 - acc: 0.6969 - val_loss: 0.6204 - val_acc: 0.6628\n",
      "Epoch 39/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5919 - acc: 0.6821 - val_loss: 0.5642 - val_acc: 0.7089\n",
      "Epoch 40/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5794 - acc: 0.6931 - val_loss: 0.5660 - val_acc: 0.7085\n",
      "Epoch 41/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5690 - acc: 0.7013 - val_loss: 0.5530 - val_acc: 0.7168\n",
      "Epoch 42/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5631 - acc: 0.7071 - val_loss: 0.5358 - val_acc: 0.7319\n",
      "Epoch 43/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5593 - acc: 0.7101 - val_loss: 0.5358 - val_acc: 0.7318\n",
      "Epoch 44/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5559 - acc: 0.7123 - val_loss: 0.5328 - val_acc: 0.7328\n",
      "Epoch 45/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5520 - acc: 0.7162 - val_loss: 0.5238 - val_acc: 0.7426\n",
      "Epoch 46/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5477 - acc: 0.7194 - val_loss: 0.5272 - val_acc: 0.7418\n",
      "Epoch 47/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5495 - acc: 0.7182 - val_loss: 0.5281 - val_acc: 0.7389\n",
      "Epoch 48/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5710 - acc: 0.6991 - val_loss: 0.5468 - val_acc: 0.7246\n",
      "Epoch 49/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5783 - acc: 0.6920 - val_loss: 0.5525 - val_acc: 0.7172\n",
      "Epoch 50/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5728 - acc: 0.6976 - val_loss: 0.5560 - val_acc: 0.7155\n",
      "Epoch 51/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5687 - acc: 0.7010 - val_loss: 0.5374 - val_acc: 0.7326\n",
      "Epoch 52/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5636 - acc: 0.7051 - val_loss: 0.5340 - val_acc: 0.7349\n",
      "Epoch 53/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5592 - acc: 0.7088 - val_loss: 0.5336 - val_acc: 0.7356\n",
      "Epoch 54/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5569 - acc: 0.7114 - val_loss: 0.5298 - val_acc: 0.7378\n",
      "Epoch 55/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5551 - acc: 0.7126 - val_loss: 0.5337 - val_acc: 0.7344\n",
      "Epoch 56/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5540 - acc: 0.7137 - val_loss: 0.5313 - val_acc: 0.7382\n",
      "Epoch 57/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5540 - acc: 0.7134 - val_loss: 0.5304 - val_acc: 0.7364\n",
      "Epoch 58/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5530 - acc: 0.7141 - val_loss: 0.5268 - val_acc: 0.7391\n",
      "Epoch 59/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5506 - acc: 0.7167 - val_loss: 0.5276 - val_acc: 0.7375\n",
      "Epoch 60/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5513 - acc: 0.7158 - val_loss: 0.5305 - val_acc: 0.7346\n",
      "Epoch 61/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5520 - acc: 0.7148 - val_loss: 0.5326 - val_acc: 0.7325\n",
      "Epoch 62/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524038/524038 [==============================] - 324s - loss: 0.5523 - acc: 0.7149 - val_loss: 0.5354 - val_acc: 0.7321\n",
      "Epoch 63/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5514 - acc: 0.7157 - val_loss: 0.5258 - val_acc: 0.7389\n",
      "Epoch 64/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5495 - acc: 0.7172 - val_loss: 0.5246 - val_acc: 0.7376\n",
      "Epoch 65/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5474 - acc: 0.7187 - val_loss: 0.5245 - val_acc: 0.7391\n",
      "Epoch 66/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5470 - acc: 0.7192 - val_loss: 0.5286 - val_acc: 0.7364\n",
      "Epoch 67/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5476 - acc: 0.7184 - val_loss: 0.5291 - val_acc: 0.7353\n",
      "Epoch 68/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5486 - acc: 0.7174 - val_loss: 0.5261 - val_acc: 0.7383\n",
      "Epoch 69/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5476 - acc: 0.7188 - val_loss: 0.5265 - val_acc: 0.7393\n",
      "Epoch 70/300\n",
      "524038/524038 [==============================] - 324s - loss: 0.5457 - acc: 0.7199 - val_loss: 0.5262 - val_acc: 0.7378\n",
      "Epoch 71/300\n",
      "147456/524038 [=======>......................] - ETA: 224s - loss: 0.5442 - acc: 0.7219"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b0b0f2d4f88e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq12_val_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq21_val_epochs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble_val_epochs_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                  callbacks=[early_stopping, model_checkpoint])\n\u001b[0m",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# See https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "best_model_path = \"../output/models/lstm_val_epochs_\" + stamp + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([q12_train, q21_train], double_train_labels,\n",
    "                 validation_data=([q12_val_epochs, q21_val_epochs], double_val_epochs_labels), \n",
    "                 epochs=300, batch_size=batch_size, shuffle=True,\n",
    "                 callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the complete train set using the number of epochs found above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO do stuff with hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-83238ed0210b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../output/models/for_validation_lstm_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstamp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq12_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq21_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1436\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                                check_batch_axis=True, batch_size=None):\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m             raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m   1297\u001b[0m                                \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                                'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "model_path = \"../output/models/for_validation_lstm_\" + stamp + '.h5'\n",
    "model.fit([q12_data, q21_data], double_labels, epochs=11, batch_size=batch_size, shuffle=True)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model and calculate logloss and accuarcy on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(model_path)\n",
    "\n",
    "predictions = model.predict([q1_val_data, q2_val_data], batch_size=batch_size, verbose=1)\n",
    "predictions += model.predict([q2_val_data, q1_val_data], batch_size=batch_size, verbose=1)\n",
    "predictions /= 2\n",
    "\n",
    "loss = log_loss(val_y, predictions)\n",
    "acc = accuracy_score(val_y, np.rint(predictions))\n",
    "\n",
    "print \"Validation scores of Lstm model\\n LogLoss: {:.4f}\\n Accuracy: {:.2f} \".format(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create roc plot and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(val_y, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "lw = 2\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k', label='Luck')\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"../output/figures/lstm_roc_plot.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
