{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Train LSTM with word2vec embeddings </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I would like to mention these two excellent posts: \n",
    "\n",
    "https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "My solution is in part based on these two guidelines.\n",
    "\n",
    "Okay now let's import all of the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from unidecode import unidecode\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "from capstone_project import utility\n",
    "from capstone_project.models import neural_nets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set important constants and load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30 # Maximum length of input for lstm the maximum number of tokens is 103 \n",
    "EMBEDDING_DIM = 300  # Length of the used word2vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_directory = \"../output/data/\"\n",
    "prefix = \"tokenized_\"\n",
    "\n",
    "train_data = utility.load_pickle(file_directory, prefix+\"train_data.pkl\")\n",
    "val_data = utility.load_pickle(file_directory, prefix+\"val_data.pkl\")  # Validation data set used to compare different classification algorithms\n",
    "train_y = train_data[\"is_duplicate\"].values\n",
    "val_y = val_data[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_tokens</th>\n",
       "      <th>q2_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355802</th>\n",
       "      <td>355802</td>\n",
       "      <td>696825</td>\n",
       "      <td>696826</td>\n",
       "      <td>Which are the best songs of Enrique Iglesias?</td>\n",
       "      <td>Which is the best song of Enrique iglesias?</td>\n",
       "      <td>1</td>\n",
       "      <td>[good, song, enrique, iglesias]</td>\n",
       "      <td>[good, song, enrique, iglesias]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2                                      question1  \\\n",
       "355802  355802  696825  696826  Which are the best songs of Enrique Iglesias?   \n",
       "\n",
       "                                          question2  is_duplicate  \\\n",
       "355802  Which is the best song of Enrique iglesias?             1   \n",
       "\n",
       "                              q1_tokens                        q2_tokens  \n",
       "355802  [good, song, enrique, iglesias]  [good, song, enrique, iglesias]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_data = train_data[:100]\n",
    "#val_data = val_data[:100]\n",
    "display(train_data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the tokenized question as input for keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 82267 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# Decode again and join strings because keras tokenizer crashes when using unicode while spacy uses it\n",
    "#q1 = train_data[\"q1_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "#q2 = train_data[\"q2_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "q1 = train_data[\"question1\"].values\n",
    "q2 = train_data[\"question2\"].values\n",
    "\n",
    "all_questions = np.concatenate([q1, q2])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_questions)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "number_words = len(word_index)+1  # Needed for embedding layer\n",
    "print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "\n",
    "q1_sequences = tokenizer.texts_to_sequences(q1)\n",
    "q2_sequences = tokenizer.texts_to_sequences(q2)\n",
    "\n",
    "q1_data = pad_sequences(q1_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(q2_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split second val and train set for validation at every epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doble the dataset size by switching the order of the questions. This is done in order to avoid symmetry issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into another training set and a second validation set \n",
    "# See: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html and\n",
    "#indices = np.arange(q1_data.shape[0])\n",
    "#np.random.shuffle(indices)\n",
    "#q1_data = q1_data[indices]\n",
    "#q2_data = q2_data[indices]\n",
    "#labels = train_y[indices]\n",
    "\n",
    "#nb_validation_samples = int(0.1 * q1_data.shape[0])\n",
    "\n",
    "# Create subset for training with early stopping\n",
    "#q1_train = q1_data[:-nb_validation_samples]\n",
    "#q2_train = q1_data[:-nb_validation_samples]\n",
    "#train_labels = labels[:-nb_validation_samples]\n",
    "\n",
    "#create validation subset that is used to validate each epoch during training\n",
    "#q1_val_epochs = q1_data[-nb_validation_samples:]\n",
    "#q2_val_epochs = q2_data[-nb_validation_samples:]\n",
    "#val_epochs_labels = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kaggle ...\n",
    "#q12_train = np.concatenate((q1_train, q2_train), axis=0)\n",
    "#q21_train = np.concatenate((q2_train, q1_train), axis=0)\n",
    "#double_train_labels = np.concatenate((train_labels, train_labels), axis=0)\n",
    "\n",
    "#q12_val_epochs = np.concatenate((q1_val_epochs, q2_val_epochs), axis=0)\n",
    "#q21_val_epochs = np.concatenate((q2_val_epochs, q1_val_epochs), axis=0)\n",
    "#double_val_epochs_labels = np.concatenate((val_epochs_labels, val_epochs_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: lstm kaggle\n",
    "VALIDATION_SPLIT = 0.1\n",
    "labels = train_y\n",
    "\n",
    "perm = np.random.permutation(len(q1_data))\n",
    "idx_train = perm[:int(len(q1_data)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(q1_data)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "q12_train = np.concatenate((q1_data[idx_train], q2_data[idx_train]), axis=0)\n",
    "q21_train = np.concatenate((q2_data[idx_train], q1_data[idx_train]), axis=0)\n",
    "double_train_labels = np.concatenate((labels[idx_train], labels[idx_train]), axis=0)\n",
    "\n",
    "q12_val_epochs = np.concatenate((q1_data[idx_val], q2_data[idx_val]), axis=0)\n",
    "q21_val_epochs = np.concatenate((q2_data[idx_val], q1_data[idx_val]), axis=0)\n",
    "double_val_epochs_labels = np.concatenate((labels[idx_val], labels[idx_val]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create correct embeddings for validation data\n",
    "#q1_validation = val_data[\"q1_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "#q2_validation = val_data[\"q2_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "q1_validation = val_data[\"question1\"].values\n",
    "q2_validation = val_data[\"question2\"].values\n",
    "\n",
    "\n",
    "q1_val_sequences = tokenizer.texts_to_sequences(q1_validation)\n",
    "q2_val_sequences = tokenizer.texts_to_sequences(q2_validation)\n",
    "\n",
    "q1_val_data = pad_sequences(q1_val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_val_data = pad_sequences(q2_val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val_labels = val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained word2vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#import os\n",
    "#\n",
    "#embeddings_index = {}\n",
    "#f = open('../data/glove.42B.300d.txt')\n",
    "#for line in f:\n",
    "#    values = line.split()\n",
    "#    word = values[0]\n",
    "#    coefs = np.asarray(values[1:], dtype='float32')\n",
    "#    embeddings_index[word] = coefs\n",
    "#f.close()\n",
    "\n",
    "#print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Credit: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "#for word, i in word_index.items():\n",
    "#    embedding_vector = embeddings_index.get(word)\n",
    "#    if embedding_vector is not None:\n",
    "#        # words not found in embedding index will be all-zeros.\n",
    "#        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters of the lstm, create the emebedding matrix and create a keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 38543\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 30, 300)       24680400    input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 230)           488520      embedding_1[0][0]                \n",
      "                                                                   embedding_1[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 460)           0           lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 460)           1840        concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 460)           0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           59008       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 128)           512         dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 128)           0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             129         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 25,230,409\n",
      "Trainable params: 548,833\n",
      "Non-trainable params: 24,681,576\n",
      "____________________________________________________________________________________________________\n",
      "230_0.30_128_0.30\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "nn_parameters = {\"max_sequence_length\": MAX_SEQUENCE_LENGTH,\n",
    "                 \"num_lstm\": 230,\n",
    "                 \"dropout_lstm\": 0.3,\n",
    "                 \"num_dense\": 128,\n",
    "                 \"dropout_dense\": 0.3}\n",
    "\n",
    "stamp = \"{}_{:.2f}_{}_{:.2f}\".format(nn_parameters[\"num_lstm\"], \n",
    "                                    nn_parameters[\"dropout_lstm\"],\n",
    "                                    nn_parameters[\"num_dense\"],\n",
    "                                    nn_parameters[\"dropout_dense\"])\n",
    "\n",
    "embedding_matrix = neural_nets.create_embedding_matrix(vec_model=word2vec_model, \n",
    "                                                       embedding_dim=EMBEDDING_DIM, \n",
    "                                                       word_index=word_index, \n",
    "                                                       number_words=number_words)\n",
    "\n",
    "model = neural_nets.create_lstm(embedding_matrix=embedding_matrix, \n",
    "                                embedding_dim=EMBEDDING_DIM, \n",
    "                                number_words=number_words, \n",
    "                                **nn_parameters)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model.summary()\n",
    "print stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the data and check the performance on the second validation set every epoch. with early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 524036 samples, validate on 58228 samples\n",
      "Epoch 1/200\n",
      "524036/524036 [==============================] - 194s - loss: 0.5770 - acc: 0.6992 - val_loss: 0.5672 - val_acc: 0.7236\n",
      "Epoch 2/200\n",
      "524036/524036 [==============================] - 193s - loss: 0.5045 - acc: 0.7466 - val_loss: 0.4817 - val_acc: 0.7818\n",
      "Epoch 3/200\n",
      "524036/524036 [==============================] - 193s - loss: 0.4721 - acc: 0.7671 - val_loss: 0.4506 - val_acc: 0.7858\n",
      "Epoch 4/200\n",
      "524036/524036 [==============================] - 193s - loss: 0.4486 - acc: 0.7816 - val_loss: 0.4212 - val_acc: 0.8008\n",
      "Epoch 5/200\n",
      "524036/524036 [==============================] - 194s - loss: 0.4303 - acc: 0.7929 - val_loss: 0.4064 - val_acc: 0.8113\n",
      "Epoch 6/200\n",
      "524036/524036 [==============================] - 193s - loss: 0.4169 - acc: 0.8011 - val_loss: 0.4067 - val_acc: 0.8068\n",
      "Epoch 7/200\n",
      "524036/524036 [==============================] - 194s - loss: 0.4045 - acc: 0.8078 - val_loss: 0.3943 - val_acc: 0.8149\n",
      "Epoch 8/200\n",
      "524036/524036 [==============================] - 193s - loss: 0.3948 - acc: 0.8140 - val_loss: 0.3886 - val_acc: 0.8199\n",
      "Epoch 9/200\n",
      "524036/524036 [==============================] - 193s - loss: 0.3860 - acc: 0.8187 - val_loss: 0.3961 - val_acc: 0.8153\n",
      "Epoch 10/200\n",
      "524036/524036 [==============================] - 194s - loss: 0.3778 - acc: 0.8235 - val_loss: 0.3775 - val_acc: 0.8264\n",
      "Epoch 11/200\n",
      "524036/524036 [==============================] - 193s - loss: 0.3704 - acc: 0.8272 - val_loss: 0.3805 - val_acc: 0.8244\n",
      "Epoch 12/200\n",
      "524036/524036 [==============================] - 193s - loss: 0.3643 - acc: 0.8300 - val_loss: 0.3814 - val_acc: 0.8232\n",
      "Epoch 13/200\n",
      "524036/524036 [==============================] - 194s - loss: 0.3590 - acc: 0.8331 - val_loss: 0.3771 - val_acc: 0.8266\n",
      "Epoch 14/200\n",
      "524036/524036 [==============================] - 194s - loss: 0.3526 - acc: 0.8373 - val_loss: 0.3693 - val_acc: 0.8322\n",
      "Epoch 15/200\n",
      "524036/524036 [==============================] - 194s - loss: 0.3479 - acc: 0.8399 - val_loss: 0.3664 - val_acc: 0.8338\n",
      "Epoch 16/200\n",
      "524036/524036 [==============================] - 193s - loss: 0.3438 - acc: 0.8416 - val_loss: 0.3754 - val_acc: 0.8305\n",
      "Epoch 17/200\n",
      "524036/524036 [==============================] - 194s - loss: 0.3392 - acc: 0.8444 - val_loss: 0.3624 - val_acc: 0.8369\n",
      "Epoch 18/200\n",
      "524036/524036 [==============================] - 193s - loss: 0.3340 - acc: 0.8477 - val_loss: 0.3639 - val_acc: 0.8351\n",
      "Epoch 19/200\n",
      "524036/524036 [==============================] - 193s - loss: 0.3307 - acc: 0.8490 - val_loss: 0.3763 - val_acc: 0.8285\n",
      "Epoch 20/200\n",
      "253952/524036 [=============>................] - ETA: 95s - loss: 0.3256 - acc: 0.8518"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-31893df4cd25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq12_val_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq21_val_epochs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble_val_epochs_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                  callbacks=[early_stopping, model_checkpoint])\n\u001b[0m",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# See https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "best_model_path = \"../output/models/lstm_val_epochs_\" + stamp + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([q12_train, q21_train], double_train_labels,\n",
    "                 validation_data=([q12_val_epochs, q21_val_epochs], double_val_epochs_labels), \n",
    "                 epochs=200, batch_size=batch_size, shuffle=True,\n",
    "                 callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the complete train set using the number of epochs found above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO do stuff with hist\n",
    "\n",
    "#524036/524036 [==============================] - 194s - loss: 0.3470 - acc: 0.8404 - val_loss: 0.3801 - val_acc: 0.8247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model and calculate logloss and accuarcy on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%model = load_model(model_path)\n",
    "\n",
    "predictions = model.predict([q1_val_data, q2_val_data], batch_size=batch_size, verbose=1)\n",
    "predictions += model.predict([q2_val_data, q1_val_data], batch_size=batch_size, verbose=1)\n",
    "predictions /= 2\n",
    "\n",
    "loss = log_loss(val_y, predictions)\n",
    "acc = accuracy_score(val_y, np.rint(predictions))\n",
    "\n",
    "print \"Validation scores of Lstm model\\n LogLoss: {:.4f}\\n Accuracy: {:.2f} \".format(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create roc plot and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(val_y, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "lw = 2\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k', label='Luck')\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"../output/figures/lstm_roc_plot.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
