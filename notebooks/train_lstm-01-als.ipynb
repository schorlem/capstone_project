{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Train LSTM with word2vec embeddings </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings and https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from unidecode import unidecode\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from capstone_project import preprocessor as pre\n",
    "from capstone_project.models import neural_nets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set important constants and load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50 # Maximum length of input for lstm the maximum number of tokens is 103 \n",
    "EMBEDDING_DIM = 300  # Length of the used word2vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_directory = \"../output/data/\"\n",
    "prefix = \"tokenized_\"\n",
    "\n",
    "train_data = pre.load_pickle(file_directory, prefix+\"train_data.pkl\")\n",
    "val_data = pre.load_pickle(file_directory, prefix+\"val_data.pkl\")  # Validation data set used to compare different classification algorithms\n",
    "train_y = train_data[\"is_duplicate\"].values\n",
    "val_y = val_data[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_data = train_data[:100]\n",
    "#val_data = val_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the tokenized question as input for keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66580 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# Decode again and join strings because keras tokenizer crashes when using unicode while spacy uses it\n",
    "q1_tokens = train_data[\"q1_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "q2_tokens = train_data[\"q2_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "all_tokens = np.concatenate([q1_tokens, q2_tokens])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_tokens)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "number_words = len(word_index)+1  # Needed for embedding layer\n",
    "print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "\n",
    "q1_sequences = tokenizer.texts_to_sequences(q1_tokens)\n",
    "q2_sequences = tokenizer.texts_to_sequences(q2_tokens)\n",
    "\n",
    "q1_data = pad_sequences(q1_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(q2_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split second val and train set for validation at every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into another training set and a second validation set \n",
    "# See: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html and\n",
    "indices = np.arange(q1_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "q1_data = q1_data[indices]\n",
    "q2_data = q2_data[indices]\n",
    "labels = train_y[indices]\n",
    "\n",
    "nb_validation_samples = int(0.1 * q1_data.shape[0])\n",
    "\n",
    "# Create subset for training with early stopping\n",
    "q1_train = q1_data[:-nb_validation_samples]\n",
    "q2_train = q1_data[:-nb_validation_samples]\n",
    "train_labels = labels[:-nb_validation_samples]\n",
    "\n",
    "#create validation subset that is used to validate each epoch during training\n",
    "q1_val_epochs = q1_data[-nb_validation_samples:]\n",
    "q2_val_epochs = q2_data[-nb_validation_samples:]\n",
    "val_epochs_labels = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doble the dataset size by switching the order of the questions. This is done in order to avoid symmetry issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kaggle ...\n",
    "q12_train = np.concatenate((q1_train, q2_train), axis=0)\n",
    "q21_train = np.concatenate((q2_train, q1_train), axis=0)\n",
    "double_train_labels = np.concatenate((train_labels, train_labels), axis=0)\n",
    "\n",
    "q12_val_epochs = np.concatenate((q1_val_epochs, q2_val_epochs), axis=0)\n",
    "q21_val_epochs = np.concatenate((q2_val_epochs, q1_val_epochs), axis=0)\n",
    "double_val_epochs_labels = np.concatenate((val_epochs_labels, val_epochs_labels), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create correct emebeddings for validation data\n",
    "q1_val_tokens = val_data[\"q1_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "q2_val_tokens = val_data[\"q2_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "\n",
    "q1_val_sequences = tokenizer.texts_to_sequences(q1_val_tokens)\n",
    "q2_val_sequences = tokenizer.texts_to_sequences(q2_val_tokens)\n",
    "\n",
    "q1_val_data = pad_sequences(q1_val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_val_data = pad_sequences(q2_val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val_labels = val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained word2vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat lstm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 30612\n"
     ]
    }
   ],
   "source": [
    "nn_parameters = {\"max_sequence_length\": MAX_SEQUENCE_LENGTH,\n",
    "                 \"num_lstm\": 200,\n",
    "                 \"dropout_lstm\": 0.25,\n",
    "                 \"num_dense\": 100,\n",
    "                 \"dropout_dense\": 0.25}\n",
    "\n",
    "stamp = \"{}_{:2f}_{}_{:.2f}\".format(nn_parameters[\"num_lstm\"], \n",
    "                                    nn_parameters[\"dropout_lstm\"],\n",
    "                                    nn_parameters[\"num_dense\"],\n",
    "                                    nn_parameters[\"dropout_dense\"])\n",
    "\n",
    "embedding_matrix = neural_nets.create_embedding_matrix(word2vec_model, EMBEDDING_DIM, word_index, number_words)\n",
    "model = neural_nets.create_lstm(embedding_matrix, EMBEDDING_DIM, number_words, **nn_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " fit the train data with early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 50, 300)       19974300    input_3[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 200)           400800      embedding_2[0][0]                \n",
      "                                                                   embedding_2[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 400)           0           lstm_2[0][0]                     \n",
      "                                                                   lstm_2[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 400)           0           concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 400)           1600        dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 100)           40100       batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 100)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 100)           400         dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             101         batch_normalization_4[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 20,417,301\n",
      "Trainable params: 442,001\n",
      "Non-trainable params: 19,975,300\n",
      "____________________________________________________________________________________________________\n",
      "200_0.250000_100_0.25\n",
      "Train on 524038 samples, validate on 58226 samples\n",
      "Epoch 1/200\n",
      "524038/524038 [==============================] - 288s - loss: 0.5382 - acc: 0.7284 - val_loss: 0.5463 - val_acc: 0.7127\n",
      "Epoch 2/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.5225 - acc: 0.7392 - val_loss: 0.5221 - val_acc: 0.7374\n",
      "Epoch 3/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.5087 - acc: 0.7492 - val_loss: 0.5054 - val_acc: 0.7537\n",
      "Epoch 4/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.4963 - acc: 0.7571 - val_loss: 0.4987 - val_acc: 0.7588\n",
      "Epoch 5/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.4852 - acc: 0.7646 - val_loss: 0.4968 - val_acc: 0.7651\n",
      "Epoch 6/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.4736 - acc: 0.7713 - val_loss: 0.4905 - val_acc: 0.7679\n",
      "Epoch 7/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.4625 - acc: 0.7774 - val_loss: 0.4897 - val_acc: 0.7679\n",
      "Epoch 8/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.4521 - acc: 0.7837 - val_loss: 0.4881 - val_acc: 0.7715\n",
      "Epoch 9/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.4423 - acc: 0.7894 - val_loss: 0.4883 - val_acc: 0.7748\n",
      "Epoch 10/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.4326 - acc: 0.7940 - val_loss: 0.4879 - val_acc: 0.7771\n",
      "Epoch 11/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.4243 - acc: 0.7978 - val_loss: 0.4899 - val_acc: 0.7767\n",
      "Epoch 12/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.4158 - acc: 0.8029 - val_loss: 0.4905 - val_acc: 0.7775\n",
      "Epoch 13/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.4082 - acc: 0.8067 - val_loss: 0.4947 - val_acc: 0.7776\n",
      "Epoch 14/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.4011 - acc: 0.8106 - val_loss: 0.4945 - val_acc: 0.7814\n",
      "Epoch 15/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.3945 - acc: 0.8132 - val_loss: 0.4940 - val_acc: 0.7794\n",
      "Epoch 16/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.3884 - acc: 0.8166 - val_loss: 0.4965 - val_acc: 0.7803\n",
      "Epoch 17/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.3817 - acc: 0.8200 - val_loss: 0.4995 - val_acc: 0.7784\n",
      "Epoch 18/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.3759 - acc: 0.8231 - val_loss: 0.5058 - val_acc: 0.7825\n",
      "Epoch 19/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.3725 - acc: 0.8247 - val_loss: 0.5126 - val_acc: 0.7831\n",
      "Epoch 20/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.3679 - acc: 0.8281 - val_loss: 0.5048 - val_acc: 0.7826\n",
      "Epoch 21/200\n",
      "524038/524038 [==============================] - 286s - loss: 0.3629 - acc: 0.8304 - val_loss: 0.5072 - val_acc: 0.7827\n"
     ]
    }
   ],
   "source": [
    "# See https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model.summary()\n",
    "print stamp\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)\n",
    "best_model_path = \"../output/models/lstm_val_epochs_\" + stamp + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([q12_train, q21_train], double_train_labels,\n",
    "                 validation_data=([q12_val_epochs, q21_val_epochs], double_val_epochs_labels), \n",
    "                 epochs=200, batch_size=4096, shuffle=True,\n",
    "                 callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the complete train set using the number of epochs found above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 100 input samples and 291132 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-74b8ec629fba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../output/models/lstm_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstamp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq1_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1436\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1321\u001b[0m                           \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0m_check_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[1;32m   1325\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    233\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mset_y\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mset_w\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         raise ValueError('Sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 100 input samples and 291132 target samples."
     ]
    }
   ],
   "source": [
    "model_path = \"../output/models/lstm_\" + stamp + '.h5'\n",
    "model.fit([q1_data, q2_data], train_y, epochs=56, batch_size=8192, shuffle=True,)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model and calculate logloss and accuarcy on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(model_path)\n",
    "\n",
    "predictions = model.predict([q1_val_data, q2_val_data], batch_size=4096, verbose=1)\n",
    "predictions += model.predict([q2_val_data, q1_val_data], batch_size=4096, verbose=1)\n",
    "predictions /= 2\n",
    "\n",
    "loss = log_loss(val_y, predictions)\n",
    "acc = accuracy_score(val_y, np.rint(predictions))\n",
    "\n",
    "print \"Validation scores of Lstm model\\n LogLoss: {:.4f}\\n Accuracy: {:.2f} \".format(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create roc plot and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(val_y, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "lw = 2\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k', label='Luck')\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"../output/figures/lstm_roc_plot.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
