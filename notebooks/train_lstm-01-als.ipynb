{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Train LSTM with word2vec embeddings </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings and https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from unidecode import unidecode\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from capstone_project import preprocessor as pre\n",
    "from capstone_project.models import neural_nets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set important constants and load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50 # Maximum length of input for lstm the maximum number of tokens is 103 \n",
    "EMBEDDING_DIM = 300  # Length of the used word2vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory = \"../output/data/\"\n",
    "prefix = \"tokenized_\"\n",
    "\n",
    "train_data = pre.load_pickle(file_directory, prefix+\"train_data.pkl\")\n",
    "val_data = pre.load_pickle(file_directory, prefix+\"val_data.pkl\")  # Validation data set used to compare different classification algorithms\n",
    "train_y = train_data[\"is_duplicate\"].values\n",
    "val_y = val_data[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_data = train_data[:100]\n",
    "#val_data = val_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the tokenized question as input for keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66580 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# Decode again and join strings because keras tokenizer crashes when using unicode while spacy uses it\n",
    "q1_tokens = train_data[\"q1_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "q2_tokens = train_data[\"q2_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "all_tokens = np.concatenate([q1_tokens, q2_tokens])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_tokens)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "number_words = len(word_index)+1  # Needed for embedding layer\n",
    "print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "\n",
    "q1_sequences = tokenizer.texts_to_sequences(q1_tokens)\n",
    "q2_sequences = tokenizer.texts_to_sequences(q2_tokens)\n",
    "\n",
    "q1_data = pad_sequences(q1_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(q2_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split second val and train set for validation at every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into another training set and a second validation set \n",
    "# See: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html and\n",
    "indices = np.arange(q1_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "q1_data = q1_data[indices]\n",
    "q2_data = q2_data[indices]\n",
    "labels = train_y[indices]\n",
    "\n",
    "nb_validation_samples = int(0.1 * q1_data.shape[0])\n",
    "\n",
    "# Create subset for training with early stopping\n",
    "q1_train = q1_data[:-nb_validation_samples]\n",
    "q2_train = q1_data[:-nb_validation_samples]\n",
    "train_labels = labels[:-nb_validation_samples]\n",
    "\n",
    "#create validation subset that is used to validate each epoch during training\n",
    "q1_val_epochs = q1_data[-nb_validation_samples:]\n",
    "q2_val_epochs = q2_data[-nb_validation_samples:]\n",
    "val_epochs_labels = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doble the dataset size by switching the order of the questions. This is done in order to avoid symmetry issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kaggle ...\n",
    "q12_data = np.concatenate((q1_data, q2_data), axis=0)\n",
    "q21_data = np.concatenate((q2_data, q1_data), axis=0)\n",
    "double_labels = np.concatenate((labels, labels), axis=0)\n",
    "\n",
    "q12_train = np.concatenate((q1_train, q2_train), axis=0)\n",
    "q21_train = np.concatenate((q2_train, q1_train), axis=0)\n",
    "double_train_labels = np.concatenate((train_labels, train_labels), axis=0)\n",
    "\n",
    "q12_val_epochs = np.concatenate((q1_val_epochs, q2_val_epochs), axis=0)\n",
    "q21_val_epochs = np.concatenate((q2_val_epochs, q1_val_epochs), axis=0)\n",
    "double_val_epochs_labels = np.concatenate((val_epochs_labels, val_epochs_labels), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create correct embeddings for validation data\n",
    "q1_val_tokens = val_data[\"q1_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "q2_val_tokens = val_data[\"q2_tokens\"].apply(lambda x: unidecode(u\" \".join(x))).values\n",
    "\n",
    "q1_val_sequences = tokenizer.texts_to_sequences(q1_val_tokens)\n",
    "q2_val_sequences = tokenizer.texts_to_sequences(q2_val_tokens)\n",
    "\n",
    "q1_val_data = pad_sequences(q1_val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_val_data = pad_sequences(q2_val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val_labels = val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained word2vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat lstm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 30612\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_14 (InputLayer)            (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)          (None, 50, 300)       19974300    input_13[0][0]                   \n",
      "                                                                   input_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                    (None, 200)           400800      embedding_7[0][0]                \n",
      "                                                                   embedding_7[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 400)           0           lstm_7[0][0]                     \n",
      "                                                                   lstm_7[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNor (None, 400)           1600        concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)             (None, 400)           0           batch_normalization_19[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 512)           205312      dropout_19[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNor (None, 512)           2048        dense_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)             (None, 512)           0           batch_normalization_20[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 256)           131328      dropout_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNor (None, 256)           1024        dense_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)             (None, 256)           0           batch_normalization_21[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 1)             257         dropout_21[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 20,716,669\n",
      "Trainable params: 740,033\n",
      "Non-trainable params: 19,976,636\n",
      "____________________________________________________________________________________________________\n",
      "200_0.40_[512, 256]_0.40\n"
     ]
    }
   ],
   "source": [
    "nn_parameters = {\"max_sequence_length\": MAX_SEQUENCE_LENGTH,\n",
    "                 \"num_lstm\": 200,\n",
    "                 \"dropout_lstm\": 0.4,\n",
    "                 \"num_dense\": [512, 256],\n",
    "                 \"dropout_dense\": 0.4}\n",
    "\n",
    "stamp = \"{}_{:.2f}_{}_{:.2f}\".format(nn_parameters[\"num_lstm\"], \n",
    "                                    nn_parameters[\"dropout_lstm\"],\n",
    "                                    nn_parameters[\"num_dense\"],\n",
    "                                    nn_parameters[\"dropout_dense\"])\n",
    "\n",
    "embedding_matrix = neural_nets.create_embedding_matrix(word2vec_model, EMBEDDING_DIM, word_index, number_words)\n",
    "\n",
    "model = neural_nets.create_lstm(embedding_matrix, EMBEDDING_DIM, number_words, **nn_parameters)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model.summary()\n",
    "print stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " fit the train data with early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 524038 samples, validate on 58226 samples\n",
      "Epoch 1/200\n",
      "524038/524038 [==============================] - 300s - loss: 0.6454 - acc: 0.6504 - val_loss: 0.6195 - val_acc: 0.6339\n",
      "Epoch 2/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.5794 - acc: 0.6959 - val_loss: 0.6002 - val_acc: 0.6479\n",
      "Epoch 3/200\n",
      "524038/524038 [==============================] - 300s - loss: 0.5617 - acc: 0.7105 - val_loss: 0.5569 - val_acc: 0.7064\n",
      "Epoch 4/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.5483 - acc: 0.7210 - val_loss: 0.5266 - val_acc: 0.7425\n",
      "Epoch 5/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.5381 - acc: 0.7285 - val_loss: 0.5148 - val_acc: 0.7504\n",
      "Epoch 6/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.5286 - acc: 0.7356 - val_loss: 0.5072 - val_acc: 0.7550\n",
      "Epoch 7/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.5209 - acc: 0.7412 - val_loss: 0.5009 - val_acc: 0.7578\n",
      "Epoch 8/200\n",
      "524038/524038 [==============================] - 300s - loss: 0.5146 - acc: 0.7450 - val_loss: 0.4993 - val_acc: 0.7605\n",
      "Epoch 9/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.5078 - acc: 0.7499 - val_loss: 0.4944 - val_acc: 0.7631\n",
      "Epoch 10/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.5014 - acc: 0.7547 - val_loss: 0.4909 - val_acc: 0.7652\n",
      "Epoch 11/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4957 - acc: 0.7579 - val_loss: 0.4889 - val_acc: 0.7681\n",
      "Epoch 12/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4903 - acc: 0.7617 - val_loss: 0.4849 - val_acc: 0.7700\n",
      "Epoch 13/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4850 - acc: 0.7651 - val_loss: 0.4837 - val_acc: 0.7699\n",
      "Epoch 14/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4799 - acc: 0.7686 - val_loss: 0.4817 - val_acc: 0.7722\n",
      "Epoch 15/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4747 - acc: 0.7714 - val_loss: 0.4825 - val_acc: 0.7734\n",
      "Epoch 16/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4697 - acc: 0.7744 - val_loss: 0.4788 - val_acc: 0.7750\n",
      "Epoch 17/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4663 - acc: 0.7763 - val_loss: 0.4785 - val_acc: 0.7752\n",
      "Epoch 18/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4612 - acc: 0.7793 - val_loss: 0.4797 - val_acc: 0.7761\n",
      "Epoch 19/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4574 - acc: 0.7814 - val_loss: 0.4745 - val_acc: 0.7777\n",
      "Epoch 20/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4539 - acc: 0.7840 - val_loss: 0.4808 - val_acc: 0.7777\n",
      "Epoch 21/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4499 - acc: 0.7863 - val_loss: 0.4799 - val_acc: 0.7782\n",
      "Epoch 22/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4467 - acc: 0.7881 - val_loss: 0.4769 - val_acc: 0.7791\n",
      "Epoch 23/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4433 - acc: 0.7901 - val_loss: 0.4769 - val_acc: 0.7807\n",
      "Epoch 24/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4404 - acc: 0.7912 - val_loss: 0.4804 - val_acc: 0.7812\n",
      "Epoch 25/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4376 - acc: 0.7932 - val_loss: 0.4773 - val_acc: 0.7809\n",
      "Epoch 26/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4339 - acc: 0.7953 - val_loss: 0.4778 - val_acc: 0.7810\n",
      "Epoch 27/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4314 - acc: 0.7964 - val_loss: 0.4810 - val_acc: 0.7822\n",
      "Epoch 28/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4286 - acc: 0.7986 - val_loss: 0.4825 - val_acc: 0.7815\n",
      "Epoch 29/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4272 - acc: 0.7990 - val_loss: 0.4770 - val_acc: 0.7825\n",
      "Epoch 30/200\n",
      "524038/524038 [==============================] - 299s - loss: 0.4242 - acc: 0.8012 - val_loss: 0.4761 - val_acc: 0.7845\n"
     ]
    }
   ],
   "source": [
    "# See https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "best_model_path = \"../output/models/lstm_val_epochs_\" + stamp + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([q12_train, q21_train], double_train_labels,\n",
    "                 validation_data=([q12_val_epochs, q21_val_epochs], double_val_epochs_labels), \n",
    "                 epochs=200, batch_size=4096, shuffle=True,\n",
    "                 callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the complete train set using the number of epochs found above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-83238ed0210b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../output/models/for_validation_lstm_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstamp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq12_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq21_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1436\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andre/anaconda3/envs/capstone_project/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                                check_batch_axis=True, batch_size=None):\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m             raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m   1297\u001b[0m                                \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                                'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "model_path = \"../output/models/for_validation_lstm_\" + stamp + '.h5'\n",
    "model.fit([q12_data, q21_data], double_labels, epochs=11, batch_size=4096, shuffle=True)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model and calculate logloss and accuarcy on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(model_path)\n",
    "\n",
    "predictions = model.predict([q1_val_data, q2_val_data], batch_size=4096, verbose=1)\n",
    "predictions += model.predict([q2_val_data, q1_val_data], batch_size=4096, verbose=1)\n",
    "predictions /= 2\n",
    "\n",
    "loss = log_loss(val_y, predictions)\n",
    "acc = accuracy_score(val_y, np.rint(predictions))\n",
    "\n",
    "print \"Validation scores of Lstm model\\n LogLoss: {:.4f}\\n Accuracy: {:.2f} \".format(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create roc plot and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(val_y, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "lw = 2\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k', label='Luck')\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"../output/figures/lstm_roc_plot.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
