{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Train LSTM with word2vec embeddings </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings and https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#import h5py\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from unidecode import unidecode\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from capstone_project import preprocessor as pre\n",
    "from capstone_project.models import neural_nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some necessary constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50 # Maximum length of input for lstm the maximum number of tokens is 103 \n",
    "EMBEDDING_DIM = 300  # Length of the used word2vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_directory = \"../output/data/\"\n",
    "prefix = \"tokenized_\"\n",
    "\n",
    "train_data = pre.load_pickle(file_directory, prefix+\"train_data.pkl\")\n",
    "#val_data = pre.load_pickle(file_directory, prefix+\"val_data.pkl\")  # Validation data set used to compare different classification algorithms\n",
    "train_y = train_data[\"is_duplicate\"].values\n",
    "#val_y = val_data[\"is_duplicate\"].values\n",
    "\n",
    "\n",
    "train_data =  train_data[:100]\n",
    "train_y = train_y[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the tokenized question as input for keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 617 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# Decode again and join strings because keras tokenizer crashes when using unicode while spacy uses it\n",
    "q1_tokens = train_data[\"q1_tokens\"].apply(lambda x: unidecode(\" \".join(x))).values\n",
    "q2_tokens = train_data[\"q2_tokens\"].apply(lambda x: unidecode(\" \".join(x))).values\n",
    "all_tokens = np.concatenate([q1_tokens, q2_tokens])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_tokens)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "number_words = len(word_index)+1  # Needed for embedding layer\n",
    "print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "\n",
    "q1_sequences = tokenizer.texts_to_sequences(q1_tokens)\n",
    "q2_sequences = tokenizer.texts_to_sequences(q2_tokens)\n",
    "\n",
    "\n",
    "q1_data = pad_sequences(q1_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(q2_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split second val and train set for validation at every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a second validation set see: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "indices = np.arange(q1_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "q1_data = q1_data[indices]\n",
    "q2_data = q2_data[indices]\n",
    "labels = train_y[indices]\n",
    "\n",
    "nb_validation_samples = int(0.1 * q1_data.shape[0])\n",
    "\n",
    "q1_train = q1_data[:-nb_validation_samples]\n",
    "q2_train = q1_data[:-nb_validation_samples]\n",
    "train_labels = labels[:-nb_validation_samples]\n",
    "\n",
    "q1_val = q1_data[-nb_validation_samples:]\n",
    "q2_val = q2_data[-nb_validation_samples:]\n",
    "val_labels = labels[-nb_validation_samples:]\n",
    "\n",
    "#TODO\n",
    "#data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "#data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "#labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "#data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "#data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "#labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained word2vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 50\n"
     ]
    }
   ],
   "source": [
    "nn_parameters = {\"max_sequence_length\": MAX_SEQUENCE_LENGTH,\n",
    "                 \"num_lstm\": 200,\n",
    "                 \"dropout_lstm\": 0.25,\n",
    "                 \"num_dense\": 100,\n",
    "                 \"dropout_dense\": 0.25}\n",
    "\n",
    "stamp = \"{}_{:2f}_{}_{:.2f}\".format(nn_parameters[\"num_lstm\"], \n",
    "                                    nn_parameters[\"dropout_lstm\"],\n",
    "                                    nn_parameters[\"num_dense\"],\n",
    "                                    nn_parameters[\"dropout_dense\"])\n",
    "\n",
    "embedding_matrix = neural_nets.create_embedding_matrix(word2vec_model, EMBEDDING_DIM, word_index, number_words)\n",
    "model = neural_nets.create_lstm(embedding_matrix, EMBEDDING_DIM, number_words, **nn_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " fit the train data with early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 50, 300)       185400      input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 200)           400800      embedding_1[0][0]                \n",
      "                                                                   embedding_1[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 400)           0           lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 400)           0           concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 400)           1600        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           40100       batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 100)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 100)           400         dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             101         batch_normalization_2[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 628,401\n",
      "Trainable params: 442,001\n",
      "Non-trainable params: 186,400\n",
      "____________________________________________________________________________________________________\n",
      "200_0.250000_100_0.25\n",
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/200\n",
      "90/90 [==============================] - 1s - loss: 1.3063 - acc: 0.6667 - val_loss: 1.1321 - val_acc: 0.60000.\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 1s - loss: 0.6437 - acc: 0.7000 - val_loss: 0.7019 - val_acc: 0.7000\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 1s - loss: 0.5480 - acc: 0.7778 - val_loss: 0.8801 - val_acc: 0.6000\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 1s - loss: 0.4857 - acc: 0.7667 - val_loss: 0.8664 - val_acc: 0.6000\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 1s - loss: 0.4033 - acc: 0.8444 - val_loss: 0.6625 - val_acc: 0.7000\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 1s - loss: 0.3123 - acc: 0.8667 - val_loss: 0.6655 - val_acc: 0.6000\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 1s - loss: 0.3415 - acc: 0.8778 - val_loss: 0.5826 - val_acc: 0.7000\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 1s - loss: 0.2708 - acc: 0.8889 - val_loss: 0.5072 - val_acc: 0.8000\n",
      "Epoch 9/200\n",
      "90/90 [==============================] - 1s - loss: 0.2005 - acc: 0.9333 - val_loss: 0.5689 - val_acc: 0.6000\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 1s - loss: 0.2664 - acc: 0.8556 - val_loss: 0.5793 - val_acc: 0.6000\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - 1s - loss: 0.2630 - acc: 0.8778 - val_loss: 0.4788 - val_acc: 0.6000\n",
      "Epoch 12/200\n",
      "90/90 [==============================] - 1s - loss: 0.1771 - acc: 0.9333 - val_loss: 0.7480 - val_acc: 0.7000\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 1s - loss: 0.1135 - acc: 0.9556 - val_loss: 0.8865 - val_acc: 0.7000\n",
      "Epoch 14/200\n",
      "90/90 [==============================] - 1s - loss: 0.2327 - acc: 0.8889 - val_loss: 0.8923 - val_acc: 0.7000\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 1s - loss: 0.1236 - acc: 0.9556 - val_loss: 0.9219 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# See https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model.summary()\n",
    "print stamp\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "best_model_path = \"../output/models/lstm_val_\" + stamp + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([q1_train, q2_train], train_labels,\n",
    "                 validation_data=([q1_val, q2_val], val_labels), \n",
    "                 epochs=200, batch_size=8, shuffle=True,\n",
    "                 callbacks=[early_stopping, model_checkpoint]) #batch_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 [==============================] - 1s - loss: 0.2020 - acc: 0.9300     \n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 1s - loss: 0.1506 - acc: 0.9500     \n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 1s - loss: 0.2325 - acc: 0.9100     \n"
     ]
    }
   ],
   "source": [
    "model_path = \"../output/models/lstm_\" + stamp + '.h5'\n",
    "model.fit([q1_data, q2_data], train_y, epochs=3, batch_size=8, shuffle=True,) #batch_size = 2048\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.training.Model object at 0x7f22567d0dd0>\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
