{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from capstone_project import preprocessor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the current working directory for python is the capstone_project/notebook folder\n",
    "data = pd.read_pickle(\"../output/data/train_data.pkl\")\n",
    "data.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "test = nlp(unicode(\"I'll do that. This is better.\", \"utf-8\"))\n",
    "for token in test:\n",
    "    print token, token.lemma, token.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = data.iloc[:, 3:-1].values, data.iloc[:, -1].values\n",
    "print type(y), y[:10]\n",
    "test = data['is_duplicate'].values\n",
    "print type(test), test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data[data.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data.add(data[[\"question1\",\"question2\"]].applymap(preprocess.tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = preprocessor.create_features(data)\n",
    "display(new_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "'The sun is shining',\n",
    "'The weather is sweet',\n",
    "'The sun is shining and the weather is sweet'])\n",
    "#bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy as sp\n",
    "\n",
    "tfidf1 = TfidfVectorizer(tokenizer=preprocessor.tokenize)\n",
    "#test = tfidf.fit_transform(data[\"question1\"][:10])\n",
    "X = tfidf1.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print test, \"\\n\", test.toarray()[0]\n",
    "#print sp.sparse.hstack(docs)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X,y=[0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None,\n",
    "                                tokenizer=preprocessor.tokenize)\n",
    "df = data\n",
    "\n",
    "tfidf.fit(pd.concat([df['question1'], df['question2']]))\n",
    "\n",
    "tfidfs_q1 = tfidf.transform(df['question1'])\n",
    "tfidfs_q2 = tfidf.transform(df['question2'])\n",
    "\n",
    "# Stack horizontally since the words in q1 and q2 should be arranged in separate columns\n",
    "tfidf_q1_q2 = hstack([tfidfs_q1, tfidfs_q2])\n",
    "\n",
    "#clf = LogisticRegression()\n",
    "#clf.fit(X,y=data.iloc[:, -1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/software/anaconda3/envs/capstone_project/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<363915x62463 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 2915884 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf = preprocessor.TfidfTransformer()\n",
    "clf = LogisticRegression()\n",
    "\n",
    "y = data[\"is_duplicate\"].values\n",
    "\n",
    "pipe = Pipeline([('tfidf', tfidf), ('clf', clf)])\n",
    "\n",
    "pipe.fit(data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tfidf', TfidfTransformer())"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps[0]\n",
    "#pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]] \n",
      "[1 1 0 0 1 0 1 0 0 0]\n",
      "Pipeline(steps=[('tfidf', TfidfTransformer())]) [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/software/anaconda3/envs/capstone_project/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "test = pipe.transform(data.iloc[0:2]).toarray()\n",
    "test2 = pipe.predict(data.iloc[0:10])\n",
    "print test, \"\\n\", test2\n",
    "\n",
    "pipe2 = Pipeline(pipe.steps[:-1])\n",
    "test2 = pipe2.transform(data.iloc[0:3]).toarray()\n",
    "print pipe2, test2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
